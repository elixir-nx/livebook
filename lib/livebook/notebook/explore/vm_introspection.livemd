# Fun with VM introspection

## Introduction

In this notebook we manually establish connection to a running node,
and then we try to retrieve and plot some interesting information
about the system.

## Setup

We are definitely gonna plot some data in this notebook,
so let's add `:vega_lite` and `:kino` for that.

```elixir
Mix.install([
  {:vega_lite, "~> 0.1.0"},
  {:kino, "~> 0.1.0"}
])
```

```elixir
alias VegaLite, as: Vl
```

## Connecting to a remote node

This time we will connect to a running node manually instead of
injecting Livebook there, because we may not want to run `Mix.install/1`
inside it. This will also give you a better idea of how you can
inspect a remote node in general.

The first thing we need is a separate Elixir node. For this example,
you can do so by opening up a new notebook, since Livebook
automatically starts each notebook as a remote node.

In practice, you may want to start an existing Elixir system,
such as by running the following in your production app:

```
elixir --sname my_app -S mix TASK
```

Or by connecting to a production assembled via
[`mix release`](https://hexdocs.pm/mix/Mix.Tasks.Release.html).

Once you start a new notebook, you can find the node name
and cookie by running this:

<!-- livebook:{"force_markdown":true} -->

```elixir
IO.puts node()
IO.puts Node.get_cookie()
```

Now, let's paste these in the inputs below:

<!-- livebook:{"livebook_object":"cell_input","name":"Node","type":"text","value":""} -->

<!-- livebook:{"livebook_object":"cell_input","name":"Cookie","type":"text","value":""} -->

```elixir
node =
  IO.gets("Node: ")
  |> String.trim()
  |> String.to_atom()

cookie =
  IO.gets("Cookie: ")
  |> String.trim()
  |> String.to_atom()

Node.set_cookie(node, cookie)
true = Node.connect(node)
```

Having successfully connected, let's try spawning a process
on the remote node!

```elixir
Node.spawn(node, fn ->
  IO.inspect(node())
end)
```

From the result of `node/1` it's clear that the function was evaluated
remotely, but note that we still get the standard output back.

## LiveDashboard

[LiveDashboard](https://github.com/phoenixframework/phoenix_live_dashboard)
is a great tool for getting information and metrics about a running system
and you can embed it into your Phoenix application very easily. In fact
even Livebook does that!

To leverage that, we first need to ensure the remote node is visible
to the Livebook server, but this may not be the case at this point!

**Why?**

By default Erlang nodes create a fully connected mesh, meaning that
each node is directly connected to all other nodes.
However, the default Livebook runtime is started as a *hidden* node
for better isolation and consequently its connections are not reflected
to other nodes. That's the current state:

```
(Livebook server) <---> (Livebook runtime) <---> (Remote node)
```

**How?**

So we "are" in `Livebook runtime` and our task is to connect
`Livebook server` with `Remote node`.

In fact, we already know how to connect to the remote node, we did that earlier.
The question is how to make `Livebook server` do the same.

First we need to determine what is the node name of `Livebook server`.
Since we are connected to this node, it's easy to check!

```elixir
[livebook_app_node | _] = Node.list(:hidden)
```

We already saw `Node.set_cookie/2` and `Node.connect/1` in action,
and we also know how to spawn a process in another node using `Node.spawn/2`.
Let's put this together!

```elixir
Node.spawn(livebook_app_node, fn ->
  # This code is evaluated in the Livebook server node
  Node.set_cookie(node, cookie)
  Node.connect(node)
end)
```

Now go to [the dashboard](/dashboard) and check out the select
in the upper right corner. If the connection was successfull, you
should be able to pick the desired node and see its details.

## Inspecting processes

In fact, we can link to particular process instances inside the LiveDashboard
by using the URL format `/{node}/processes?info={pid}`. Let's create a helper for that:

```elixir
defmodule Utils do
  @doc """
  Returns a URL to the given process page in LiveDashboard.
  """
  @spec dashboard_url(pid()) :: String.t()
  def dashboard_url(pid) do
    [livebook_app_node | _] = Node.list(:hidden)

    # Note: the PID needs to be formatted relatively to
    # the Livebook server node, so we call inspect/1 there
    "#" <> pid_str = :rpc.call(livebook_app_node, Kernel, :inspect, [pid])

    "/dashboard/#{node(pid)}/processes?info=#{pid_str}"
  end
end
```

Awesome, we already got the idea of how the nodes are connected
and can see information about the node within LiveDashboard.
Now we are going to extract some information from the running node on our own!

Let's get the list of all processes in the system:

```elixir
remote_pids = :rpc.call(node, Process, :list, [])
```

Wait, but what is this `:rpc.call/4` thing? 🤔

Previously we used `Node.spawn/2` to run a process on the other node
and we used the `IO` module to get some output. However, now
we actually care about the resulting value of `Process.list/0`!

We could still use `Node.spawn/2` to send us the results, which
we would `receive`, but doing that over and over can be quite tedious.
Fortunatelly, `:rpc.call/4` does essentially that - evaluates the given
function on the remote node and returns its result.

Now, let's gather more information about each process 🕵️

```elixir
processes =
  Enum.map(remote_pids, fn pid ->
    # Extract interesting process information
    info = :rpc.call(node, Process, :info, [pid, [:reductions, :memory, :status]])
    # The result of inspect(pid) is relative to the node
    # where it was called, that's why we call it on the remote node
    pid_inspect = :rpc.call(node, Kernel, :inspect, [pid])

    %{
      pid: pid_inspect,
      dashboard_url: Utils.dashboard_url(pid),
      reductions: info[:reductions],
      memory: info[:memory],
      status: info[:status]
    }
  end)
```

Having all that data, we can now visualize it on a scatter plot!

```elixir
Vl.new(width: 600, height: 400)
|> Vl.data_from_values(processes)
|> Vl.mark(:point, tooltip: true)
|> Vl.encode_field(:x, "reductions", type: :quantitative, scale: [type: "log", base: 10])
|> Vl.encode_field(:y, "memory", type: :quantitative, scale: [type: "log", base: 10])
|> Vl.encode_field(:color, "status", type: :nominal)
|> Vl.encode_field(:tooltip, "pid", type: :nominal)
|> Vl.encode_field(:href, "dashboard_url", type: :nominal)
```

From the plot we can easily see which processes do the most work
and take the most memory. Also, you can click individual processes to see them
in LiveDashboard!

## Tracking memory usage

There's a very simple way to determine current memory usage in the VM:

```elixir
:erlang.memory()
```

We can use `Kino.VegaLite.periodically/4` to create a self-updating
plot of memory usage over time on the remote node!

```elixir
widget =
  Vl.new(width: 600, height: 400, padding: 20)
  |> Vl.repeat(
    [layer: ["total", "processes", "atom", "binary", "code", "ets"]],
    Vl.new()
    |> Vl.mark(:line)
    |> Vl.encode_field(:x, "iter", type: :quantitative, title: "Measurement")
    |> Vl.encode_repeat(:y, :layer, type: :quantitative, title: "Memory usage (MB)")
    |> Vl.encode(:color, datum: [repeat: :layer], type: :nominal)
  )
  |> Kino.VegaLite.start()
  |> Kino.render()

Kino.VegaLite.periodically(widget, 200, 1, fn i ->
  point =
    :rpc.call(node, :erlang, :memory, [])
    |> Enum.map(fn {type, bytes} -> {type, bytes / 1_000_000} end)
    |> Map.new()
    |> Map.put(:iter, i)

  Kino.VegaLite.push(widget, point, window: 1000)
  {:cont, i + 1}
end)
```

Unless you connected to a production node, the memory usage
most likely doesn't change, so to emulate some spikes you can
run the following code in the remote node:

**Binary usage**

<!-- livebook:{"force_markdown":true} -->

```elixir
x = Enum.reduce(1..10_000, [], fn i, acc ->
  [String.duplicate("cat", i) | acc]
end)
```

**ETS usage**

<!-- livebook:{"force_markdown":true} -->

```elixir
tid = :ets.new(:users, [:set, :public])

for i <- 1..1_000_000 do
  :ets.insert(tid, {i, "User #{i}"})
end
```
